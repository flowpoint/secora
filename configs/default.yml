logdir: "/root/secora_output/"
checkpoint_dir: "/root/secora_output/"

# the name has to follow the naming scheme
# see secora/train.py
name: 'run_secora_t0_utc00'
seed: 42
max_checkpoints: 10

num_gpus: 1 #'auto'
amp: disable #default
cuda_graphs: False

preprocess_cores: 10
preprocess_mode: concat
max_input_tokens: 256
languages:
  - all

epochs: 2
shards: 16
warmup_batches: 10000

finetune_mode: all

#model_name: 'microsoft/codebert-base'
model_name: 'roberta-base'
optimizer: adam
lr_schedule: constant

learning_rate: 1e-5
batch_size: 2
grad_accum: 64 # counted in batches
temp: 0.05
dropout: 0.1
grad_clip: 0.0

embedding_size: 768
top_k:  1000
