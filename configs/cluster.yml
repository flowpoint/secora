hostname: localhost
port: 12355

num_gpus: 'auto'

name: 'cluster_run_name'

batch_size: 8
infer_batch_size: 8

seed: 42
epochs: 1
shards: 20

grad_accum: 64 # counted in batches

warmup_batches: 10000
temp: 0.05

embedding_size: 128
top_k:  5

logdir: "../cluster_output"
checkpoint_dir: "../cluster_output"

max_checkpoints: 10

model_name: 'microsoft/codebert-base'
learning_rate: 1e-5

finetune_mode: all

languages:
    - python


preprocess_cores: 10
preprocess_mode: concat

max_input_tokens: 256

optimizer: adam

precision: mixed
