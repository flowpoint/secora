logdir: "./output"
checkpoint_dir: "./output"

# the name has to follow the naming scheme
# see secora/train.py
name: 'debug_secora_t0_utc00'
seed: 42
max_checkpoints: 0

num_gpus: 1 #'auto'
amp: default
cuda_graphs: False

preprocess_cores: 10
preprocess_mode: concat
max_input_tokens: 256
languages:
  - python

epochs: 2
shards: 2
warmup_batches: 10

finetune_mode: all

#model_name: 'microsoft/codebert-base'
model_name: 'roberta-base'
optimizer: adam
lr_schedule: constant

learning_rate: 1e-5
batch_size: 16
grad_accum: 1 # counted in batches
temp: 0.05
dropout: 0.1
grad_clip: 0.0

embedding_size: 768
top_k:  1000
