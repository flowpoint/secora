logdir: "/tmp/secora_output/"
checkpoint_dir: "/tmp/secora_output/"

# the name has to follow the naming scheme
# see secora/train.py
training_run_id: '0'
seed: 42
max_checkpoints: 10

num_gpus: 1 #'auto'
amp: disable #default
cuda_graphs: False

preprocess_cores: 22
preprocess_mode: concat
max_input_tokens: 256
languages:
  - all

epochs: 1
shards: 64
warmup_batches: 100

finetune_mode: all

#model_name: 'microsoft/codebert-base'
#model_name: 'roberta-base'
model_name: 'distilroberta-base'
optimizer: adam
lr_schedule: constant

learning_rate: 1e-5
batch_size: 16
grad_accum: 1 # counted in batches
grad_cache: 32
temp: 0.05
dropout: 0.1
grad_clip: 0.0

embedding_size: 768
top_k:  1000
